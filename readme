Visualization of vanilla MARDM:
    python sample.py --name MARDM_SiT_XL --text_prompt "A person is waving his left hand."


Visualization of source motion:
    python /root/workspace/MARDM/external/GMR/scripts/vis_npz_motion.py --npz_path /root/workspace/MARDM/data/semi_synthetic_v1_segments/11995_HAND_SNATCH-4_1_motion.npz

Visualize VAE comparison (original vs reconstructed):
    python vis_AE_comparison.py --npz_path checkpoints/beat_v2/AE/test_results/sample_0000.npz --output_path videos/ae_comparison.mp4 --robot g1_branco

Preprocess whisper feature of BEAT_v2:
    python utils/batch_whisper_features.py

Preprocess whisper feature of semi_synthetic_v1:
   
    python utils/batch_whisper_features.py --input_dir /root/workspace/MARDM/data/semi_synthetic_v1 --num_gpus 8 --num_workers 4
    

Compute mean and std of BEAT_v2 and semi_synthetic_v1_segments:
    python utils/cal_mean_std.py
    Note: This will calculate mean and std for both BEAT_v2 and semi_synthetic_v1_segments datasets.
          Results will be saved to their respective data directories (Mean.npy and Std.npy).

Train VAE on mixed dataset (BEAT_v2 + semi_synthetic_v1_segments) (8 GPUs):
    torchrun --nproc_per_node=8 --master_port=29501 train_AE.py --name AE --dataset_name mixed --batch_size 256 --epoch 50 --lr_decay 0.05 --distributed
    Note: batch_size is per GPU, so total batch size = 256 * 8 = 2048
    Note: Use --master_port to specify port (default: 29501, avoids 29500). Must use torchrun's --master_port, not environment variable.
    Note: Mixed dataset combines BEAT_v2 and semi_synthetic_v1_segments datasets for training.

Train VAE on mixed dataset (BEAT_v2 + semi_synthetic_v1_segments) (single GPU):
    python train_AE.py --name AE --dataset_name mixed --batch_size 256 --epoch 50 --lr_decay 0.05

Evaluate VAE on mixed dataset (BEAT_v2 + semi_synthetic_v1_segments):
    python evaluation_AE.py --name AE --dataset_name mixed --model AE_Model --batch_size 256 --window_size 180

Test VAE on mixed dataset and generate comparison videos with audio:
    python test_AE_mixed.py \
        --name AE \
        --model AE_Model \
        --batch_size 256 \
        --window_size 300 \
        --num_samples_per_dataset 10 \
        --sample_strategy diverse \
        --generate_videos \
        --num_video_samples 10 \
        --robot_type g1_brainco \
        --motion_fps 60 \
        --rate_limit
    
    Note: This script samples 10 samples from BEAT_v2 and 10 samples from semi_synthetic_v1_segments (total 20 samples).
          Videos are generated with original audio merged from the source datasets.
          Use --sample_strategy diverse (default) to ensure different motions are selected.
          Options: sequential (order), diverse (spaced), random (completely random)


Train MARDM:
   
    # For audio-to-motion dataset (beat_v2) - 8 GPU training:
    torchrun --master_port 29503 --nproc_per_node=8 train_MARDM.py --name MARDM_SiT_XL_beat_v2 --model "MARDM-SiT-XL" --dataset_name beat_v2 --batch_size 16 --ae_name AE --milestones 20000 --max_motion_length 300 --distributed 

Test MARDM (beat_v2):
   
    # 1. 在test set上展示效果:
    python test_beat_v2.py --mode testset --name MARDM_SiT_XL_beat_v2 --model "MARDM-SiT-XL" --dataset_name beat_v2 --ae_name AE --num_test_samples 100 --batch_size 16

    # 2. 从给定的audio wav文件生成motion并渲染成视频:
    python test_beat_v2.py --mode audio --name MARDM_SiT_XL_beat_v2 --model "MARDM-SiT-XL" --dataset_name beat_v2 --ae_name AE --audio_path /root/workspace/MARDM/data/BEAT_v2/1/1_wayne_0_2_2.wav --whisper_model base

Test MARDM (mixed):
   
    # 1. 在test set上展示效果:
    python test_beat_v2.py --mode testset --name MARDM_SiT_XL_mixed --model "MARDM-SiT-XL" --dataset_name mixed --ae_name AE --num_test_samples 100 --batch_size 16

    # 2. 从给定的audio wav文件生成motion并渲染成视频（支持长音频）:
    python test_beat_v2.py --mode audio --name MARDM_SiT_XL_mixed --model "MARDM-SiT-XL" --dataset_name mixed --ae_name AE --audio_path /root/workspace/MARDM/data/BEAT_v2/1/1_wayne_0_2_2.wav --whisper_model base
    
    # 3. 从长音频文件生成motion（自动分段处理）:
    python test_beat_v2.py --mode audio --name MARDM_SiT_XL_mixed --model "MARDM-SiT-XL" --dataset_name mixed --ae_name AE --audio_path /path/to/long_audio.wav --whisper_model base

Test MARDM (semi_synthetic):
   
    # 1. 在test set上展示效果:
    python test_beat_v2.py --mode testset --name MARDM_SiT_XL_semi_synthetic --model "MARDM-SiT-XL" --dataset_name semi_synthetic --ae_name AE --num_test_samples 100 --batch_size 16

    # 2. 从给定的audio wav文件生成motion并渲染成视频（支持长音频）:
    python test_beat_v2.py --mode audio --name MARDM_SiT_XL_semi_synthetic --model "MARDM-SiT-XL" --dataset_name semi_synthetic --ae_name AE --audio_path /root/workspace/MARDM/data/BEAT_v2/1/1_wayne_0_2_2.wav --whisper_model base



Process semi_synthetic_v1 to segments:

 python3 utils/preprocess_semi_synthetic.py \
  --data_dir /root/workspace/MARDM/data/semi_synthetic_v1 \
  --motion_stat_dir /root/workspace/MARDM/data/motion_stat \
  --output_dir /root/workspace/MARDM/data/semi_synthetic_v1_segments \
  --clip_version ViT-B/32 \
  --device cuda


Train MARDM on mixed data:
    # 在混合数据集上训练 MARDM (8 GPU):
    torchrun --master_port 29503 --nproc_per_node=8 train_MARDM.py \
    --name MARDM_SiT_XL_mixed \
    --model "MARDM-SiT-XL" \
    --dataset_name mixed \
    --batch_size 16 \
    --ae_name AE \
    --milestones 20000 \
    --max_motion_length 300 \
    --distributed

Train MARDM on semi-synthetic data only:
    # 只在semi-synthetic数据集上训练 MARDM (8 GPU):
    torchrun --master_port 29503 --nproc_per_node=8 train_MARDM.py \
    --name MARDM_DDPM_XL_semi_synthetic_caption \
    --model "MARDM-DDPM-XL" \
    --dataset_name semi_synthetic \
    --batch_size 64 \
    --ae_name AE \
    --milestones 20000 \
    --max_motion_length 300 \
    --distributed \
    --epoch 1000

    # 使用text CLIP feature噪声注入进行训练（数据增强）:
    torchrun --master_port 29503 --nproc_per_node=8 train_MARDM.py \
    --name MARDM_DDPM_XL_semi_synthetic_noise \
    --model "MARDM-DDPM-XL" \
    --dataset_name semi_synthetic \
    --batch_size 64 \
    --ae_name AE \
    --milestones 20000 \
    --max_motion_length 300 \
    --text_clip_noise_prob 0.5 \
    --text_clip_noise_normalize \
    --distributed \
    --epoch 1000




